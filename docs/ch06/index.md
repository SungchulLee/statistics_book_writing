# Chapter 6: General Statistical Estimation

## Overview

This chapter formalizes the question every statistician faces: given data, how do we construct good estimators, and how do we measure "good"? We develop the theoretical machinery for evaluating estimator quality — bias, variance, mean squared error, consistency, and efficiency — and then present two systematic methods for constructing estimators: maximum likelihood and the method of moments.

## Learning Objectives

After completing this chapter, you should be able to:

- Define bias, variance, and mean squared error (MSE) of an estimator
- Explain the bias–variance tradeoff and its implications for estimation
- Distinguish between consistency and unbiasedness
- Construct the likelihood function for a given model
- Derive maximum likelihood estimators for common distributions
- Apply the method of moments to obtain estimators
- Compare MLE and MoM estimators for the same parameter

## Prerequisites

- Chapter 3: Random variables, expectation, and variance
- Chapter 4: Common probability distributions (Normal, Bernoulli, Poisson, Exponential)
- Chapter 5: Sampling distributions and the concept of a statistic as a random variable

## Sections

| Section | Topic |
|---|---|
| 6.1 | Estimator Quality: Bias, Variance, MSE, Consistency, Efficiency |
| 6.2 | Estimation Methods: Likelihood Function, MLE, and Method of Moments |
| 6.3 | Code |
| 6.4 | Exercises |
